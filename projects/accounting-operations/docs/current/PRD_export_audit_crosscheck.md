# PRD — Export Revenue Audit Cross‑Check (2023–2024)

## 1. Purpose and Scope

This document describes the end‑to‑end flow for validating that export revenue reported in quarterly PVT Excel reports matches the underlying bank statements for business accounts.

Scope:
- Years: 2023–2024.
- Reports: final quarterly PVT reports in `YYYY_QN.xlsx` format (e.g. `2023_Q3.xlsx`).
- Statements: business accounts under `/Users/m/Documents/accounting/bank`.
- Code: lives in this repo `/Users/m/ai/projects/accounting-operations`, treating the `/Users/m/Documents/accounting` tree as data only.

Out of scope:
- Modifying bank statement files or quarter report files.
- Changing how statements are generated by the bank.

## 2. High‑Level Flow

The system runs as a three‑stage pipeline, all driven by small Python scripts plus tests and feature files:

1. **Export row extraction (reports → JSON)**  
   - Input: final quarterly reports, e.g. `/Users/m/Documents/accounting/reports/final 2023-2024/2023_Q1.xlsx`.  
   - Behaviour: for each `YYYY_QN.xlsx`:
     - Read the main PVT table on sheet `Лист1`:
       - Header row: row `10` (`A10..H10`).
       - Data row: row `11` (`A11..H11`) — the row that feeds the export amount.
       - Export row: row `17` (`D17` label "Экспорт", `F17` amount).
     - Build a JSON entry with:
       - `file`, `sheet`.
       - `export` (label cell, label text, amount cell, numeric amount).
       - `source_row` indexed by columns A..H, each with `cell`, `header`, `value`.
   - Output (example): `metadata/exports_2023_2024.json` in this repo.

2. **Export transaction extraction (statements → JSON)**  
   - Input: bank statements under `/Users/m/Documents/accounting/bank`:
     - Inventory from `bank/metadata/statements.toml`.
     - Files in `renamed-statements/` and `renamed-statements-2025/`.
   - Behaviour (planned):
     - For each business account statement XLSX:
       - Read all transaction rows and normalize:
         - `statement_file`, `account_id`, `statement_period_start/end`.
         - `txn_date`, `value_date`, `amount`, `currency`, `description/comment`.
       - Mark potential *export* rows using configurable heuristics:
         - Direction (incoming credit).
         - Account/currency matching export customers.
         - Optional text patterns (e.g. counterparty names, contract/invoice numbers).
   - Output (planned): `bank/metadata/export_statement_rows.json`.

3. **Quarter‑level reconciliation (reports JSON ↔ statements JSON)**  
   - Input:
     - `metadata/exports_2023_2024.json` (report side).
     - `bank/metadata/export_statement_rows.json` (statement side).
   - Behaviour (planned):
     - For each quarter entry:
       - Determine the time window and other filters from the export row:
         - Quarter label, month/year cell, optional recognition date.
       - Select matching statement rows:
         - Within appropriate date range.
         - Correct account(s) and currency.
       - Sum the matched amounts and compare to the report export amount.
     - Produce a reconciliation result per quarter:
       - Reported export amount.
       - Sum of matched statement transactions.
       - Difference.
       - List of matched transactions (for human inspection).
   - Output (planned): `metadata/export_reconciliation_2023_2024.json` and/or a Markdown summary.

All three stages will be covered by feature files under `features/` and unit tests under `tests/`.

## 3. Interfaces and Commands

### 3.1. Export row extraction (implemented)

Script:
- `metadata/export_quarter_reports.py`

CLI:
- From `/Users/m/ai/projects/accounting-operations`:
  ```bash
  python metadata/export_quarter_reports.py \
    --reports-root "/Users/m/Documents/accounting/reports/final 2023-2024" \
    --out metadata/exports_2023_2024.json
  ```

Behaviour:
- Scans `--reports-root` for `*.xlsx`.
- For each workbook:
  - Prefers sheet named `Лист1`, falls back to the first sheet.
  - Reads row 10 (headers) and row 11 (data) for columns A..H.
  - Reads `D17` and `F17` as the export label and amount cell references.
  - Sets the export amount from `F11` (the revenue cell) to avoid dependence on recalculation.
- Writes one JSON array entry per workbook with:
  - `file`, `sheet`, `export.*`, `source_row.index`, `source_row.cells`.

Tests:
- `tests/test_export_quarter_reports.py` (using `openpyxl` to create temporary workbooks).
- Run:
  ```bash
  cd /Users/m/ai/projects/accounting-operations
  python -m unittest tests.test_export_quarter_reports
  ```

### 3.2. Statement export transaction extraction (planned)

Proposed script:
- `bank/metadata/export_statement_rows.py` (under the data repo, already mapped via `DATA_ROOT` usage).

CLI example:
- From `/Users/m/ai/projects/accounting-operations`:
  ```bash
  DATA_ROOT=/Users/m/Documents/accounting/bank \
  python bank/metadata/export_statement_rows.py \
    --out bank/metadata/export_statement_rows.json
  ```

Expected behaviour:
- Read `DATA_ROOT/metadata/statements.toml` to know which statement files exist.
- For each business account statement:
  - Open XLSX with `openpyxl` in read‑only mode.
  - Iterate transaction rows, normalizing relevant columns and matching a simple schema:
    - `statement_file`, `account_id`, `period_start`, `period_end`
    - `row_index`, `txn_date`, `value_date`, `amount`, `currency`, `description`
  - Optionally tag rows as export candidates based on filters (to be refined during implementation).
- Write all normalized rows (or only tagged export rows) to JSON.

Tests (to be added):
- Synthetic small statements with:
  - Known export‑like transactions.
  - Non‑export transactions (noise).
- Assert:
  - Only expected rows are included/flagged as export.
  - Dates and amounts are parsed and serialized correctly.

### 3.3. Reconciliation (planned)

Proposed script:
- `metadata/export_reconciliation.py`

CLI example:
- From `/Users/m/ai/projects/accounting-operations`:
  ```bash
  python metadata/export_reconciliation.py \
    --exports metadata/exports_2023_2024.json \
    --statements /Users/m/Documents/accounting/bank/metadata/export_statement_rows.json \
    --out metadata/export_reconciliation_2023_2024.json
  ```

Expected behaviour:
- For each quarter in `exports_2023_2024.json`:
  - Derive a target time window and possibly account/currency filters.
  - Select candidate statement rows from `export_statement_rows.json`.
  - Sum amounts and compare to the export amount.
  - Emit a result object that includes:
    - Quarter id (file name, quarter label).
    - Reported export amount.
    - Sum of matched transactions.
    - Difference.
    - Matched transactions (with statement file and row references).
- The reconciliation JSON can be rendered as a Markdown summary if needed.

Tests (to be added):
- Unit tests on reconciliation logic using small in‑memory JSON fixtures.
- One high‑level integration test that:
  - Uses synthetic exports JSON and synthetic statements JSON.
  - Asserts correct classification of:
    - Exact match.
    - Missing transactions.
    - Over‑reported amounts.

## 4. Non‑Functional Requirements

- **Read‑only**:
  - Scripts must never modify `bank` or `reports` Excel files; all operations use read‑only `openpyxl` access.
- **Deterministic, repeatable**:
  - Given the same reports and statements, re‑running the scripts must produce identical JSON.
- **BDD and tests first**:
  - Each major script (`export_quarter_reports.py`, statement extraction, reconciliation) must have:
    - A feature file in `features/`.
    - Unit tests under `tests/`.
  - Code is considered “ready” only when tests pass.
- **Clear separation of data and code**:
  - `/Users/m/Documents/accounting` is treated as data only.
  - `/Users/m/ai/projects/accounting-operations` contains all scripts, tests, and documentation.

## 5. Current Status

- Implemented:
  - `metadata/export_quarter_reports.py` (report → JSON extraction).
  - Tests for export extraction: `tests/test_export_quarter_reports.py` (green).
  - Feature description: `features/export_quarter_reports.feature`.
  - Story: `docs/current/export_quarter_reports_story.md`.
- Planned:
  - Statement export transaction extractor.
  - Reconciliation script and integration test.
  - Optional Markdown summary of reconciliation results.

## 6. ADRs (Architecture Decision Records)

**ADR‑001 — Separate data and code trees**  
- Decision: keep bank data and report files under `/Users/m/Documents/accounting`, and put all code, tests, and PRD/feature docs in `/Users/m/ai/projects/accounting-operations`.  
- Rationale: avoids accidental modification of delivered or archived data; allows treating the code repo as version‑controlled logic only.  
- Consequences: scripts must use absolute paths or env vars (e.g. `DATA_ROOT`, `--reports-root`) to reach the data.

**ADR‑002 — JSON as the interchange format**  
- Decision: use JSON (not YAML) for intermediate artefacts: export rows, statement rows, reconciliation results.  
- Rationale: Python standard library support, easy to inspect, already used by other tools in the environment.  
- Consequences: downstream tools should expect UTF‑8 JSON with `ensure_ascii=False`.

**ADR‑003 — Use openpyxl; no recalculation in this path**  
- Decision: use `openpyxl` to read Excel files; do not call LibreOffice/`recalc.py` from this pipeline.  
- Rationale: for audit purposes we rely on the values already saved in the workbooks; recalculation belongs to the modelling phase, not to reconciliation.  
- Consequences: if a workbook has inconsistent formulas vs. saved values, this pipeline will reflect the saved values (same as the PDFs sent to counterparties).

**ADR‑004 — Fixed PVT layout assumptions for 2023–2024**  
- Decision: assume the quarterly PVT reports share a stable layout:
  - Sheet `Лист1`.
  - Headers in row 10, data row in row 11, export label in `D17`, export amount in `F17`.  
- Rationale: this matches the actual 2023–2024 reports and simplifies extraction logic.  
- Consequences: if future templates change, we will either:
  - Add configuration for row/column indices, or
  - Support multiple templates with explicit detection.

**ADR‑005 — BDD‑style specifications + unit tests as gate**  
- Decision: any new script in this flow must be introduced together with:
  - A feature file describing behaviour in `features/`.
  - At least one unit test module in `tests/` that exercises core logic.  
- Rationale: keeps the pipeline understandable and safe to run before an audit; lets you review behaviour by reading the feature files first.  
- Consequences: a small amount of upfront structure, but easier maintenance and confidence when regenerating artefacts before sending documents to auditors.

