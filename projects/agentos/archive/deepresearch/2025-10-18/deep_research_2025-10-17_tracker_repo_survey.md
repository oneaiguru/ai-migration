**ChatGPT:**

# Deep Research Task 3 — **Subscription‑Mode Usage Tracker (Claude Code & Codex CLI)**

## A) **Tracker System Repo Research**

**Summary:** We identified several open-source projects that address _subscription-mode usage tracking_ for Anthropic Claude Code and OpenAI Codex/ChatGPT. The top picks include a **Claude usage monitor** with real-time session tracking, a cross-platform **usage analyzer** that parses local JSONL logs for both Claude and Codex, and a lightweight **multi-armed bandit** library for optimizing tool selection. These projects avoid API billing endpoints (using local data and CLIs), store usage data in flat files (JSON/JSONL), and offer features like 5-hour session detection, weekly cap estimates, and cost calculations. Below we detail the top 3 repositories, followed by additional mentions, gaps, and integration plans.

**Top Recommendations:**

1. **Repo: Claude-Code-Usage-Monitor**  
    **URL:** https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor  
    **Stars:** 5.5k   **Last Update:** 2025‑10 (v3.0.0)   **License:** MIT  
    **What it does:** Real-time **Claude Code usage monitor** with live CLI/TUI output. It parses Claude’s local JSONL conversation logs (`~/.claude/projects/`) and tracks 5-hour session windows, token usage, and plan-specific limits[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Claude%20Code%20operates%20on%20a,hour%20rolling%20session%20window%20system). It provides warnings when you near limits and estimates reset times.  
    **Relevant features:**
    
    * ✅ **5‑hour block detection:** Automatically recognizes session start times and enforces the 5h window for each Claude Code session[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Claude%20Code%20operates%20on%20a,hour%20rolling%20session%20window%20system). It can handle overlapping sessions and multiple rolling windows.
        
    * ✅ **Weekly cap awareness:** Updated for Anthropic’s weekly limits (configurable); auto-detects if usage patterns exceed Pro tier and suggests switching to higher tier mode[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=The%20auto).
        
    * ✅ **Plan-specific allowances:** Built-in token thresholds for Pro vs. Max plans (e.g. ~19k tokens for Pro, 88k for Max5, 220k for Max20 per 5h)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=), and cost estimates (shows cost “burn” in USD).
        
    * ✅ **Live burn rate & predictions:** Continuously calculates token burn rate (tokens/min) and projects when the current session will hit its limit[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=), giving users time-to-reset and usage velocity indicators.
        
    * ❌ **OpenAI support:** Focused on Claude; does not parse ChatGPT exports or OpenAI usage (though similar methods could be added).  
        **Code quality:** Python (clean, modular) • Docs: Good (README with examples, CLI usage, plan tables) • Activity: Active (recent commits, v3.0.0 with new features)  
        **Key files:** `src/claude_monitor/core.py` (session & token parser), `src/claude_monitor/ui/` (Rich TUI components[GitHub](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor/blob/06f0fe11e694b8619f63f8b0db10dbdc5e7e5a44/src/claude_monitor/ui/components.py#L80-L89)[GitHub](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor/blob/06f0fe11e694b8619f63f8b0db10dbdc5e7e5a44/src/claude_monitor/ui/components.py#L100-L108)), `src/claude_monitor/config.yaml` (plan token limits)  
        **Reusability:** Fork **YES** (we can reuse its Claude log parser and TUI) • Extract **YES** (modules for session detection, plan config) • Significant changes **NO** (mostly integration with our scheduler)  
        **Why it matters here:** This tool nails the Claude side: it already handles 5-hour rolling windows, token counting, and limit resets. We can fork it to ingest Claude’s local JSONL and emit normalized JSONL events for each prompt and session boundary. Its plan definitions and live progress bar will save us a lot of effort[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Claude%20Code%20operates%20on%20a,hour%20rolling%20session%20window%20system)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Plan%20Limit%20,00%20250%2B%20Machine%20learning).  
        **Recommendation:** **FORK** – Use as the foundation for Claude subscription usage tracking (integrate its parser and possibly embed its TUI for real-time monitoring).
        
2. **Repo: ccusage (Claude & Codex Usage Analyzer)**  
    **URL:** https://github.com/ryoppippi/ccusage  
    **Stars:** 8.5k   **Last Update:** 2025‑10   **License:** MIT  
    **What it does:** A comprehensive CLI tool (Node/TS) that analyzes usage from local logs for **Claude Code** and **OpenAI Codex/ChatGPT**. It reads Claude’s JSONL traces and OpenAI’s Codex logs to produce detailed reports (daily, monthly, per session) including token counts and cost in USD[github.com](https://github.com/ryoppippi/ccusage#:~:text=,Beta)[github.com](https://github.com/ryoppippi/ccusage#:~:text=,Protocol%20server%20for%20integration%20with). It supports **5-hour blocks** analysis and even real-time dashboards for active sessions[github.com](https://github.com/ryoppippi/ccusage#:~:text=,breakdown%60%20flag). (The tool is primarily CLI, but we can leverage its logic or outputs.)  
    **Relevant features:**
    
    * ✅ **Local JSONL parsing (Claude):** Scans `~/.claude/projects/**.jsonl` to aggregate token usage per day, per session, and per 5-hour block[github.com](https://github.com/ryoppippi/ccusage#:~:text=,breakdown%60%20flag). Automatically detects when a “5-hour limit reached” event closes a block and starts a new one (exposed via the `blocks` command)[github.com](https://github.com/ryoppippi/ccusage#:~:text=,live).
        
    * ✅ **OpenAI ChatGPT/Codex support:** Companion analyzer for OpenAI usage (invoked as `ccusage-codex`). It looks at `~/.codex/` logs or ChatGPT conversation data to summarize prompt counts. For ChatGPT Plus, it can incorporate message limits (reports when users hit the **“User limit reached, try again in X hours”** events)[startupspells.com](https://startupspells.com/p/ai-caps-pricing-anthropic-claude-code-vs-openai-codex-usage-limits#:~:text=,to%20upgrade%20to%20%24200%20plan). _(It’s not fully documented how it obtains ChatGPT usage; likely via conversation exports or a CLI integration. We might extract only partial logic or require users to supply the `conversations.json` export.)_
        
    * ✅ **Normalized JSON output:** All reports can be output as JSON/JSONL (`--json` flag) for programmatic use[preslav.me](https://preslav.me/2025/08/04/put-claude-code-token-usage-macos-toolbar/#:~:text=Initially%2C%20I%20considered%20scraping%20the,j)[preslav.me](https://preslav.me/2025/08/04/put-claude-code-token-usage-macos-toolbar/#:~:text=The%20result%20is%20a%20clean%2C,time%20totals). This means we can run `ccusage` as a subprocess to get structured data or mimic its data model internally. It already computes fields like `totalTokens`, `costUSD`, and can mark whether a session is active or ended[GitHub](https://github.com/b0o/dotfiles/blob/1796a4d955d8276ad306f01608e6718388d120a9/config/waybar/bin/claude-usage.py#L38-L46)[GitHub](https://github.com/b0o/dotfiles/blob/1796a4d955d8276ad306f01608e6718388d120a9/config/waybar/bin/claude-usage.py#L89-L97).
        
    * ✅ **Effective cost model:** Includes pricing for tokens under subscription. The tool tracks **cost per prompt** and per model (Opus vs Sonnet) and knows plan prices (e.g. it shows daily cost totals)[preslav.me](https://preslav.me/2025/08/04/put-claude-code-token-usage-macos-toolbar/#:~:text=ImageBy%20default%2C%20ccusage%20will%20display,for%20the%20last%20few%20days)[github.com](https://github.com/ryoppippi/ccusage#:~:text=readability%20,Protocol%20server%20for%20integration%20with). It likely uses internal tables or an online source for plan pricing; offline mode uses cached pricing data[github.com](https://github.com/ryoppippi/ccusage#:~:text=,Protocol%20server%20for%20integration%20with).
        
    * ❌ **Scheduling/bandit logic:** ccusage focuses on reporting, not on deciding which model to use next. It doesn’t include a scheduler or multi-provider optimization (that’s outside its scope).  
        **Code quality:** TypeScript (monorepo with packages) • Docs: Excellent (README, usage examples, website) • Activity: Very active (90 releases, frequent updates)  
        **Key files:** `packages/claude/src/` (Claude parsing logic), `packages/codex/src/` (Codex/ChatGPT parsing logic), `ccusage.example.json` (sample output structure[github.com](https://github.com/ryoppippi/ccusage#:~:text=ccusage)). _Note:_ Code is in TS, but we can refer to it for logic or treat the CLI as a black box tool.  
        **Reusability:** Fork **NO** (non-Python, large codebase) • Extract **PARTIAL** (we can’t directly import TS code, but can replicate its JSONL schema and maybe call it via CLI for initial data dumps) • Significant changes **N/A** (we’d likely use its output format as our standard).  
        **Why it matters here:** ccusage has done the heavy lifting to **parse usage logs** for both Claude and OpenAI. It confirms the feasibility of analyzing subscription usage without API calls. We can use its JSON output as a reference for our normalized schema (e.g. fields like tokens, cost). In particular, its **5-hour block accounting** and **Codex integration** fill critical gaps. For example, ccusage explicitly supports a “5-Hour Blocks Report” and live monitoring of session progress[github.com](https://github.com/ryoppippi/ccusage#:~:text=,breakdown%60%20flag), and it offers a separate mode for OpenAI’s Codex/ChatGPT usage[github.com](https://github.com/ryoppippi/ccusage#:~:text=ccusage%20,Analyzer)[github.com](https://github.com/ryoppippi/ccusage#:~:text=%40ccusage%2Fcodex%20,Analyzer).  
        **Recommendation:** **REFERENCE / EXTRACT** – We won’t fork it directly (different stack), but we should study its parsing algorithms and possibly call its CLI for an initial one-time import of historical data. Key extraction: adopt its JSON schema and mimic its block segmentation logic for consistency.
        
3. **Repo: Thompson (PyPI)**  
    **URL:** https://pypi.org/project/thompson/ (GitHub: _erdogant/thompson_)  
    **Stars:** N/A (PyPI package)   **Last Update:** 2025‑04   **License:** MIT  
    **What it does:** A minimal **multi-armed bandit** implementation in Python focusing on Thompson Sampling (and UCB). It provides a simple API to set up arms and update rewards[pypi.org](https://pypi.org/project/thompson/#:~:text=,armed%20bandit%20problem)[pypi.org](https://pypi.org/project/thompson/#:~:text=Documentation%20pages). We can use this or similar libraries to introduce basic statistical optimization in our usage tracker – for example, choosing between Claude vs ChatGPT based on cost-efficiency and success rates.  
    **Relevant features:**
    
    * ✅ **Thompson Sampling & UCB:** Implements Bayesian Thompson Sampling for stochastic bandits and an Upper Confidence Bound algorithm[pypi.org](https://pypi.org/project/thompson/#:~:text=,armed%20bandit%20problem). This fits our need to explore which assistant yields better “quality per cost” and then exploit the better option.
        
    * ✅ **Lightweight, file-friendly:** No external DB or complex integration; we can maintain our own reward metrics (e.g. store success/failure and cost in JSONL) and feed them into this library’s model. The state of the bandit (Beta distributions for each arm) can be saved or reinitialized from our logs.
        
    * ✅ **Confidence and early stopping:** By using Thompson Sampling, we inherently get a measure of confidence as the Beta posterior spreads narrow. We can interpret when one tool has ≥90% selection probability. (The library doesn’t have a UI, but we can observe the arm probabilities to know when to firmly prefer one tool.)
        
    * ❌ **Domain-specific logic:** The library itself doesn’t know about LLM usage or costs – we’ll have to translate our usage events into binary rewards or custom reward values (e.g. “quality_score = success/cost”). Also, it doesn’t handle scheduling or knapsack constraints directly – it’s purely for choosing an arm each trial given past rewards.  
        **Code quality:** Python (single module) • Docs: Fair (PyPI page with basic examples) • Activity: Low (algorithm is stable, last updated April 2025)  
        **Key files:** (Single file distribution) – usage via `import thompson as th` then `th.Bandit()` etc. Example on PyPI shows straightforward usage in a few lines[pypi.org](https://pypi.org/project/thompson/#:~:text=import%20thompson%20as%20th)[pypi.org](https://pypi.org/project/thompson/#:~:text=%2A%20Example%3A%20Compute%20multi,using%20Thompson).  
        **Reusability:** Fork **NO** (we can just import it) • Extract **NO** (not needed, it’s small and can be used as-is) • Significant changes **NO** (we’ll wrap it with our reward logic)  
        **Why it matters here:** To optimize tool usage under caps, we need a way to continuously learn which assistant gives more “bang for buck.” This library (or a similar open-source bandit package) lets us plug in a reward signal (e.g. did the model produce a correct answer within cost?) and will automatically bias future selections toward the better-performing model[pypi.org](https://pypi.org/project/thompson/#:~:text=,armed%20bandit%20problem). It requires minimal integration and no servers, aligning with our JSONL-and-files approach.  
        **Recommendation:** **UTILIZE** – Integrate this as our **bandit/CI module**. We will maintain a JSONL of outcomes (quality scores and costs per prompt), and use `thompson` to decide which tool to use next (Claude vs ChatGPT) based on the posterior probabilities of success per cost.
        

**Notable Mentions:**

* **cc-monitor-rs (Rust)** – A Rust-based Claude usage monitor by another developer[github.com](https://github.com/ZhangHanDong/cc-monitor-rs#:~:text=ZhangHanDong%2Fcc,2%20forks%20Branches%20Tags), inspired by Maciek’s tool. It offers a native UI using Makepad and tracks Claude’s token usage in real time. Not as feature-rich or popular, but worth noting for its approach. _License: Apache-2.0, 16 stars._
    
* **Claude-trace (Interceptor)** – A Node.js tool that intercepts Claude’s API calls to log all requests and responses to JSONL[GitHub](https://github.com/badlogic/lemmy/blob/92e4ba60328bb9e6d756f18bd5c1e2f166768a61/apps/claude-trace/src/interceptor.ts#L37-L45)[GitHub](https://github.com/badlogic/lemmy/blob/92e4ba60328bb9e6d756f18bd5c1e2f166768a61/apps/claude-trace/src/interceptor.ts#L51-L59). It’s more about debugging and capturing conversation data (including token usage metadata) than usage limits, but the JSONL it produces could be another source of truth. _Not directly needed since Claude Code already logs usage._
    
* **Anthropic `/usage` CLI output** – While not a repo, note that Claude Code’s built-in `/usage` command provides a quick summary of remaining time in the current session and weekly quota[claudelog.com](https://claudelog.com/faqs/claude-code-limit/#:~:text=%2A%205,but%20conversation%20history%20remains%20intact)[claudelog.com](https://claudelog.com/faqs/claude-code-limit/#:~:text=,limits%20reset%20every%207%20days). This isn’t programmatic, but we can parse its text if needed. (No official API for subscription usage; our tools above cover this via log parsing.)
    
* **OpenAI ChatGPT Data Export** – ChatGPT’s **`conversations.json`** export (via account settings) is a key data source for prompt counts. Some community scripts (shared on forums) convert this JSON to CSV/Markdown[reddit.com](https://www.reddit.com/r/OpenAI/comments/1i414z4/any_tips_on_making_json_conversation_archive/#:~:text=Any%20tips%20on%20making%20JSON,formatted%20markdown%20files), though we didn’t find a dedicated repo for usage stats. We’ll likely write a custom parser for this, informed by how ccusage’s Codex module expects data.
    
* **Proxies/Monitoring platforms** (Helicone, Langfuse, etc.) – These were considered but **excluded** because they target API usage. They require API keys and focus on token billing, not applicable for subscription CLI usage (and they introduce external dependencies or databases, which we’re avoiding).
    

**Gaps We Need to Fill:** Despite the strengths of the above projects, a few requirements remain unaddressed:

* **Unified JSONL schema:** We need to harmonize Claude and ChatGPT usage events into one normalized schema (as outlined). ccusage provides a model, but we must implement our own emitter in Python. For example, capturing **weekly remaining quota** for ChatGPT (which isn’t directly given by any tool) will require an estimate (perhaps initialize to known message cap and decrement).
    
* **ChatGPT session/window detection:** There is no direct concept of a 5-hour window in ChatGPT Plus; instead, usage is limited by a rolling message cap (e.g. GPT-5 messages per 3 hours) or an eventual cooldown lock[startupspells.com](https://startupspells.com/p/ai-caps-pricing-anthropic-claude-code-vs-openai-codex-usage-limits#:~:text=,to%20upgrade%20to%20%24200%20plan). We need to infer an equivalent “session” or simply log each prompt as an event with a timestamp and track daily/weekly totals. (Our system might treat each ChatGPT conversation or each day as a “session” for lack of a better grouping.)
    
* **Scheduler / knapsack logic:** None of the identified repos deals with scheduling prompts across tools under constraints. We must implement a custom **scheduler** that uses the parsed usage data to decide, e.g., “Claude session is 80% used, switch to ChatGPT for next task” or “ChatGPT daily limit hit, use Claude until it resets”. This involves modeling the 5h blocks and weekly quotas as resources – effectively a **knapsack or allocation problem** we’ll solve greedily or with simple heuristics.
    
* **Quality tracking:** We have to define and capture a `quality_score` or success metric for each prompt (perhaps via user feedback or automated tests). The bandit’s reward function will depend on this. None of the external tools inherently track success of answers – that’s domain-specific. We’ll likely incorporate hooks for the user to mark a prompt’s outcome (success/fail) which we log for the bandit.
    
* **Pricing data integration:** We should create a **`plans.yaml`** (as imagined) mapping plan names to allowances and cost. ccusage and Claude Monitor have embedded data we can reference[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=%2A%20According%20to%20third)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Plan%20Limit%20,00%20250%2B%20Machine%20learning), but we need to maintain our own copy for offline use. This includes token limits per 5h, estimated messages per week, etc., for each plan (Claude Pro/Max and ChatGPT Plus/Pro).
    
* **Cross-platform considerations:** The Claude monitor is Python but relies on Rich for TUI; ccusage is Node. We need to ensure our combined tool remains lightweight. Possibly, not all users will have Node installed for ccusage – we might have to run without it by implementing the critical parsing ourselves (especially for ChatGPT exports).
    

**Recommended Approach:** We propose to **FORK** the Claude Usage Monitor as our base, **EXTRACT** key logic from ccusage (by reimplementing in Python as needed), and **BUILD** small modules for scheduling and bandit decisions. Specifically:

* Use the Claude monitor’s parser to watch the Claude JSONL files and emit events in our normalized JSONL format (one line per prompt, plus session start/end events when limits hit). We’ll modify it to run in a headless mode that writes JSONL rather than only showing TUI, although we can preserve an option to launch the TUI for live viewing.
    
* Implement a new **OpenAI usage parser** (in Python) that ingests ChatGPT `conversations.json` exports. This will create a prompt event for each user message, include the model (we can detect GPT-5 vs GPT-3.5 by message metadata if available, or assume GPT-5 for plus users), and track the count. We’ll need to manage an internal counter for weekly usage – since OpenAI doesn’t give an explicit number, we might initialize e.g. 800 prompts/week for Plus (as per our plan YAML) and then decrement whenever we log a prompt[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=tokens%20you%20get%20per%20week,or%20per%20session). If we detect a “limit reached” message in the conversation (e.g. the assistant says “limit reached, wait X hours”), we log a `limit_reached` event for ChatGPT with the time and perhaps reset our counter after the cooldown.
    
* Develop a **scheduler module** that reads the latest usage state (e.g. how many Claude 5h sessions have been used this week, how many ChatGPT messages used, time until Claude reset, etc.) and decides which tool to allocate for the next prompt. This will consider: (a) availability (if Claude is currently in a lockout, use ChatGPT, and vice versa), (b) remaining quotas (don’t exceed weekly limits), and (c) cost efficiency. A simple heuristic is to always use the cheaper option (likely ChatGPT) until it’s unavailable, then failover to Claude – but we will refine this with bandit feedback. The **5-hour blocks** will be modeled by marking when a Claude session opens and closes; the scheduler will avoid starting a new Claude session if it knows the weekly hour cap is nearly exhausted, etc. (Likewise, it will avoid sending a prompt to ChatGPT if the user is in a cooldown period.)
    
* Integrate the **bandit** on top of the scheduler: the bandit will adjust the scheduler’s choices when both resources are available. For instance, if ChatGPT is cheaper but perhaps has lower quality on coding tasks, and the bandit learns Claude yields higher success probability, it may override the purely cost-based choice and assign more prompts to Claude up to its limits. Implementation-wise, before each new task, we will query the bandit for which tool to use (`arm = argmax(ExpectedReward = success_prob / cost)`). The bandit’s parameters get updated after each prompt is completed and we log the outcome.
    
* Everything will write to flat JSONL files: one for usage events (normalized schema) and possibly another for bandit outcomes (or we embed reward info in the same events). This makes it easy to version control and inspect, and our tools can simply append to these logs.
    

**Next Steps:**

1. **Fork & Set Up Claude Monitor:** _Dev A_ will fork the Claude-Code-Usage-Monitor repo. Remove or modularize the TUI so it can run in a daemon-like mode. Implement an output layer that writes our unified JSONL events (mapping Claude Monitor’s internal data to our schema fields like `ts, session_id, block_id, tokens_in/out, etc.`). Write unit tests with synthetic Claude logs to ensure session_open and limit_reached events are correctly emitted (use the example in Appendix AT-1).
    
2. **Implement ChatGPT Parser:** _Dev B_ will write a Python script to parse `conversations.json`. Start with the structure: iterate through each conversation and each user message. Determine model (perhaps by presence of “GPT-5” in the conversation title or using the assumption that Plus uses GPT-5 by default) and output a prompt event. Also parse system messages for any “limit reached” notifications to create `limit_reached` events. Test it with a dummy export (Appendix AT-2) and ensure it tallies message counts correctly.
    
3. **Plan & Pricing Module:** _Dev C_ will create `plans.yaml` covering Claude Pro/Max and ChatGPT Plus/Pro. Populate it with known token limits (e.g. from Anthropic support docs and community analysis[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=%2A%20According%20to%20third)[github.com](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor#:~:text=Plan%20Limit%20,00%20250%2B%20Machine%20learning)) and monthly pricing. Write a small `pricing_ledger.py` that can answer: “given tool X (claude or chatgpt) and plan Y, what’s the effective cost per prompt or per block?” and “what’s the weekly quota of prompts or sessions?”. This will be used to annotate events with `effective_cost_usd` and to initialize weekly_cap estimates.
    
4. **Scheduler & Knapsack Logic:** _Dev A and B together_ will design the scheduling policy. Likely start with a simple loop that at each new prompt checks: is Claude available (has an open session and not at limit, or can open new session?) vs is ChatGPT available (not in cooldown)? Then pick by lowest cost. Implement this in a `scheduler.py` that can be called with current usage stats. Add logic to enforce that we don’t exceed known weekly caps (e.g. if Claude has only 2 sessions left for the week, maybe reserve them for high-value prompts as indicated by bandit expected reward). This may involve a greedy knapsack where each prompt is a “value” and each tool’s usage consumes part of the weekly allowance; for now, a rule-based approach (alternate or prefer one until limit) is fine.
    
5. **Bandit Integration:** _Dev C_ will integrate the `thompson` library. Define reward: e.g. each prompt yields reward = 1 if success, 0 if failure (or a scaled score), and cost = effective_cost_usd from the event. We may use “success minus cost” or another formula for bandit rewards (or incorporate cost into success probability threshold). Start with a simple Bernoulli reward (success = 1 if the answer met quality criteria, else 0) and let cost indirectly influence the bandit because an expensive tool will yield fewer successes for the same number of opportunities if it’s used less often. We’ll test the bandit by simulating a scenario where one tool is indeed cheaper for equal quality and ensure it biases towards that tool (Appendix AT-4).
    
6. **Testing & Verification:** Combine everything and run on synthetic scenarios: simulate hitting Claude’s 5h limit, simulate ChatGPT cooldown, ensure events log correctly and scheduler switches tools appropriately. Particularly test edge cases: starting a Claude session 4h 59m before weekly reset, using two Claude sessions in parallel (should be logged separately), ChatGPT hitting the 3-hour GPT-5 message cap. Write unit tests for each module and an integration test that feeds a sequence of prompts and yields an allocated tool for each (making sure we never allocate when the resource is exhausted).
    

By following these steps, we’ll build a minimal yet comprehensive usage tracker that leverages proven open-source components and fills in the blanks for subscription-specific behavior.