**ChatGPT:**

### Finding: Claude Code’s 5‑Hour Usage Window is Rolling and Token-Based

**Provider/Plan/Model(s):** Anthropic Claude (Pro/Max, Claude Code)  
**Source:** Anthropic support & Portkey blog (2025)  
**Evidence Type:** **Tier A** – official help center confirmation; **Tier B** – expert summary  
**Exact Quote/Screenshot:** _“The core limit is a 5-hour rolling session that begins with your first prompt… all Claude.ai plans share a common usage bucket across Claude app & Claude code; Max plan just multiplies the 5-hour allowance”_[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=,hour%20allowance). _“From day one, Claude Code was released with a 5-hour rolling usage limit, enforced through token consumption – not literal clock time”_[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=From%20day%20one%2C%20Claude%20Code,com).

**Workload Context (if applicable):** N/A (applies to all interactive usage of Claude’s consumer app & CLI)  
**Measurement (if applicable):** _Window duration:_ 5 hours from first prompt; _Token allotment per window:_ ~44k (Pro), ~88k (Max 5×), ~220k (Max 20×)[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=%2A%20According%20to%20third).  
**Implications:** Within a **single 5h window**, all prompts consume a fixed token budget (roughly corresponding to 5 hours of typical usage). The “timer” doesn’t run on wall-clock alone – sending a huge prompt can end the session early by exhausting tokens. The window **resets after 5 hours elapse**, but effectively **only when you send the next message** beyond that point[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=The%20moment%20you%20run%20,%28Source%3A%20Anthropic%20Help%20Center). This means start times are under your control: e.g. if you kick off a session at 9:00, it will fully reset at 14:00, ready for a new session on your next prompt[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=Instead%2C%20your%20personal%205,you%20send%20your%20first%20message). There is **no global schedule** for everyone; each user’s 5h cycle is personal and can be strategically aligned with work periods (see overlap trick below). Crucially, _“hours” are just a token-based estimate_ – there is **no literal countdown timer**; heavy token use can terminate a session in less than 5 actual hours[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=From%20day%20one%2C%20Claude%20Code,com). The additivity assumption holds per session: using 3% of the weekly bar in window1 and 4% in window2 means ~7% total, barring minor rounding.  
**Open Questions:** Does an ongoing reply get cut off at exactly 5h mark or allowed to finish? (Likely the latter, since the limit is token-based and checked per prompt.) Anthropic hasn’t explicitly stated the grace behavior for prompts that straddle the 5h boundary – an experiment could log token usage if a reply spans the reset time.

### Finding: Users Can **Overlap Sessions** to Double Continuous Work Time

**Provider/Plan/Model(s):** Anthropic Claude (Pro tier example)  
**Source:** Independent pro-user guide (2025)  
**Evidence Type:** **Tier B** – user-derived strategy (validated by consistent behavior)  
**Exact Quote/Screenshot:** _“Claude’s 5-hour window starts when YOU start… Your personal 5-hour timer starts the moment you send your first message… Starting a session on one [platform] starts the clock for both. Fire up Claude on the web at 10:00 AM? That 5-hour window now applies to Claude Code too… **The Session Overlap Trick**: 10:00 AM send ‘Hi’ to Claude (starts Session 1)… 1:00 PM start coding; ~2:45 ‘approaching limit’… 3:00 PM Session 1 expires → Session 2 instantly available… Result: 4 hours continuous work, zero waiting”_[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=Instead%2C%20your%20personal%205,you%20send%20your%20first%20message)[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=With%20the%20trick%3A).

**Workload Context:** Claude Pro plan; coding session heavy enough to normally hit limit ~2–3h in. Trick uses a minimal dummy prompt in advance.  
**Measurement:** _Window1:_ 3h used (minus one “Hi” prompt); _Window2:_ immediately available on expiry; _Total uninterrupted time:_ ~4h vs 2h normally.  
**Implications:** This confirms the **independence of session windows**: by starting an early “sacrificial” session, a user ensures a second fresh 5h pool comes due right when the first pool would have run out[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=,hours%20work%2C%203%20hours%20waiting)[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=,hours%20continuous%20work%2C%20zero%20waiting). Critically, this does **not violate any usage rules** – the total weekly tokens consumed remains the same, but you avoid idle lockout time. It also highlights that **session start time is entirely user-controlled**[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=Instead%2C%20your%20personal%205,you%20send%20your%20first%20message). For the optimization system, this means scheduling agents’ start times in a staggered way could smooth utilization. However, overlapping windows don’t **increase** capacity; they just let you use two windows back-to-back. The **additivity** assumption still holds across windows (two consecutive 5h allotments sum their token budgets).  
**Open Questions:** If multiple features pipelines run concurrently on one account (say, two overlapping windows actively used), do they simply split the token pool or hit any concurrency limits? (Likely just shared budget – see concurrency finding below.) This technique mainly helps human productivity; for an automated scheduler, it suggests always utilizing the window fully then immediately rotating to the next.

### Finding: Claude’s Weekly Usage Caps Reset on a Fixed Schedule (Not Rolling)

**Provider/Plan/Model(s):** Anthropic Claude (all paid plans)  
**Source:** Anthropic help & user reports (2025)  
**Evidence Type:** **Tier A** – official UI/Help info; **Tier B** – user confirmation  
_Update 2025-12:_ Current Claude `/usage` shows a **single weekly bar for all models**. Any references below to separate per-model weekly meters describe legacy UI and are kept for historical context only; our active PRD and code assume a single pooled weekly cap.  
**Exact Quote/Screenshot:** _“Weekly limits: Check when your plan’s weekly usage limit resets.”_[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=,time%20remaining%20in%20the%20session). _“Our documentation… indicates that weekly usage limits should follow predetermined reset schedules… The all-models timer operates that way.”_ User reports highlighted inconsistencies between different weekly meters and suggested the behavior might have been buggy in early releases[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=Claude%E2%80%99s%20help%20agent%20for%20as,be%20consistent%20with%20both%20trackers). In one Max plan example, _“Opus weekly limit reached ∙ resets Oct 6, 1pm”_[medium.com](https://medium.com/@sevakavakians/claude-code-limit-hit-on-max-plan-what-to-do-next-60389024e0af#:~:text=Suddenly%2C%20I%20saw%20the%20warning,bottom%20right%20of%20my%20terminal) was displayed.

**Workload Context:** N/A (account-wide behavior; e.g. Oct 6 was the Monday of that week)  
**Measurement:** _Reset interval:_ 7 days (exact day/time shown in UI); _Reset mode:_ hard reset (bars drop to 0% at once).  
**Implications:** The **weekly cap** is on a **calendar cadence**, not a sliding window. All users have the same cycle (e.g. likely Monday-to-Monday or fixed day per plan start) – you can see the next reset timestamp in Settings or via `/usage`[claudelog.com](https://claudelog.com/faqs/claude-code-limit/#:~:text=%2A%205,AI). This means unused capacity **does not carry over** past the reset time. It also means heavy usage on say Day 1 can lock you out for the remainder of the week (no “rolling” relief until the scheduled reset). For planning, the system should never schedule right up against the reset with critical work – if a feature build is in progress near the weekly reset time, it may abruptly stop when the cap zeroes out. All evidence suggests resets are **hard resets**: at the moment of reset, the percent counters return to 0% for both the general and Opus bars. In-flight conversations likely see the next prompt after reset treated as new week usage. (Anthropic’s help doesn’t mention grace periods; users have reported work halting exactly when the limit hit, then resuming after the reset time[medium.com](https://medium.com/@sevakavakians/claude-code-limit-hit-on-max-plan-what-to-do-next-60389024e0af#:~:text=Now%2C%20I%20get%20this%20message%3A,%E2%80%9D).)  
**Open Questions (historical):** **Timezone and exact timing** – The UI shows the timestamp (which presumably is local to user or UTC?). E.g. Oct 6, 1pm might correspond to a fixed UTC offset. We should verify if **DST or timezones** ever skew the reset (user reports haven’t flagged issues aside from the Opus timer confusion). The reddit note suggests the **Opus weekly counter might have initially been offset** (perhaps starting count when first used), but presumably it should align with the same fixed reset – this might have been a launch bug. It’s worth running a controlled test: log usage on an account just before reset, see if one can still send a prompt at X:59 and how it’s counted post-reset.

### Finding: Anthropic Weekly Usage Quotas (Historical Two-Meter UI)

**Provider/Plan/Model(s):** Anthropic Claude (Pro vs Max plans; Sonnet 4 vs Opus 4)  
**Source:** Anthropic announcement & support  
**Evidence Type:** **Tier A** – official plan details; **Tier B** – user observations  
_Update 2025-12:_ Claude’s current `/usage` screen exposes a **single weekly bar** instead of separate “all models” vs “Opus” meters. The evidence and quotes below describe the earlier two-meter design and are preserved only as historical context; our optimizer now models weekly usage as one pooled constraint.  
**Exact Quote/Screenshot:** _“Weekly limits… for different model groups.”_[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=,time%20remaining%20in%20the%20session). Pro plan example: _“Estimated Sonnet 4 usage ~40–80 hrs/week; Opus not included. Max ($100) ~140–280 hrs/week Sonnet + ~15–35 hrs/week Opus. Max ($200) ~240–480 hrs/week Sonnet + ~24–40 hrs/week Opus.”_[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=Estimated%20Sonnet%204%20Usage)[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=). After hitting Opus cap on Max 20×, a user noted: _“Opus weekly limit reached… resets Oct 6, 1pm… Next I get: ‘Claude usage limit reached. Resets Oct 6, 1pm’… I’m on the MAX plan!”_[medium.com](https://medium.com/@sevakavakians/claude-code-limit-hit-on-max-plan-what-to-do-next-60389024e0af#:~:text=Suddenly%2C%20I%20saw%20the%20warning,bottom%20right%20of%20my%20terminal)[medium.com](https://medium.com/@sevakavakians/claude-code-limit-hit-on-max-plan-what-to-do-next-60389024e0af#:~:text=Hmm%2C%20maybe%20its%20a%20bug,limits%2C%20then%20use%20Sonnet%204).

**Workload Context:** Claude Pro ($20) can only use “Sonnet” model; Max allows Opus usage in mix. E.g. user was using Opus for planning and Sonnet for coding in a session.  
**Measurement:** _Pro:_ ~40–80h of Sonnet 4 (no Opus) per week. _Max 5×:_ ~3.5× Pro’s Sonnet hours **plus** ~20h Opus. _Max 20×:_ ~6× Pro’s Sonnet hours **plus** ~30h Opus[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=Pro%20)[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=Max%20). In user case, Opus bar hit 100% mid-week (15–35h used) while “All models” bar was ~90%, then system blocked further use of either model until reset[medium.com](https://medium.com/@sevakavakians/claude-code-limit-hit-on-max-plan-what-to-do-next-60389024e0af#:~:text=Now%2C%20I%20get%20this%20message%3A,%E2%80%9D).  
**Implications (historical two-meter UI):** **Independence & coupling:** In the older design, the **All-Other-Models bar** tracked usage of Claude’s standard models (e.g. Sonnet 4, Haiku 3.5) and had a larger budget, while the **Opus bar** was a smaller, separate budget dedicated to the high-cost model (Claude Opus 4.x). Using Opus **appeared** to count against both bars: it depleted the Opus-specific quota and (most evidence suggested) also incremented the general “all models” percentage in parallel. Conversely, using only lower models left the Opus meter at 0%[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=Question%20does%20opus%20count%20towards,Only%E2%80%9D%20usage%20still%20shows%200). The system would **halt usage when either bar hit 100%** – effectively requiring that **both** quotas had headroom to continue. For example, if you exhausted your high-end model hours early in the week, even if you had Sonnet hours left, Claude Code could refuse any request (as experienced above). Likewise, a Pro user could hit 100% on the general bar (with no high-end usage) and be locked out for the week[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=Question%20does%20opus%20count%20towards,Only%E2%80%9D%20usage%20still%20shows%200). This suggested modeling weekly usage as separate constraints. With the unified weekly bar, our current optimizer instead treats weekly capacity as a **single pool** and ignores any historical two-meter quirks.  
**Open Questions (no longer modeled):** Anthropic has not published an exact “coupling coefficient” between any separate weekly meters that may have existed historically. Given the move to a single weekly bar and our reliance on direct measurement via `/usage`, we no longer attempt to model per-model weekly quotas separately; the single weekly meter is the only weekly constraint the system tracks.

### Finding: Larger Inputs, Files, and Tool Uses Burn Proportionally More of the Usage Bars

**Provider/Plan/Model(s):** Anthropic Claude (Claude Code, all plans)  
**Source:** Anthropic Help Center  
**Evidence Type:** **Tier A** – official usage guidelines  
**Exact Quote/Screenshot:** _“Additional factors that affect your usage limits include: Message length, File attachment size, Current conversation length, Tool usage (e.g. web search), Model choice, Artifact creation… Content in projects is cached and doesn't count against your limits when reused… Similar prompts you use frequently are partially cached.”_[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=Additional%20factors%20that%20affect%20your,usage%20limits%20include)[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=Our%20system%20also%20includes%20caching,helps%20you%20optimize%20your%20limits).

**Workload Context:** E.g. coding with large context: uploading a 500 KB code file vs a 5 KB snippet, or using the browser/search tool intensively in a conversation.  
**Measurement:** No fixed numeric increments given (Anthropic does not show token counts), but qualitatively, a single long message with a 10MB attachment could consume as much quota as dozens of small prompts. Cached content re-used in multiple prompts incurs cost only once.  
**Implications:** This confirms that **usage is essentially token-based**: anything that increases token processing (longer prompts, bigger attached files, longer conversation histories to carry) will **increase the % burn** on the weekly and session bars[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=Additional%20factors%20that%20affect%20your,usage%20limits%20include). For our system, that means the conversion from “agent actions” to `% of cap used” is sensitive to **prompt size and context**. If a methodology involves feeding an entire codebase context repeatedly, it will chew through the budget faster than one that breaks tasks into smaller, cached chunks. The mention of **tool usage** implies that using Claude’s built-in web search or filesystem tools also counts toward the same limits[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=) (likely because tool results are fed back into the model, consuming tokens). We must assume **monotonicity**: more content = more tokens = higher % – there’s no scenario where adding content reduces the bar (aside from caching not charging twice). The system’s additivity assumption holds at least piecewise: 2 prompts of 5K tokens each will use roughly double the quota of one 5K-token prompt (subject to caching if overlapping content). There could be slight non-linearity if very large contexts incur overhead, but Anthropic doesn’t indicate any nonlinear pricing—just straight token accumulation.  
**Open Questions:** How exactly does _“current conversation length”_ factor in? Possibly, each new message must resend the recent conversation context (as is typical with ChatML), meaning later prompts implicitly consume more tokens. This suggests diminishing returns in very long threads (your 50th message is carrying a huge context). The caching helps if using Projects: e.g. uploading a library file once and referring to it by name later uses cache rather than tokens. We should identify if the CLI offers a way to pre-cache context (Projects) for coding. Also, **attachments** – are they fully tokenized (e.g. a 1MB text file ~ 500k tokens)? If so, that could wipe out a huge chunk of the 5h allowance in one go. We might need to chunk large docs or rely on retrieval augmentation to avoid hitting context limits and burning quota.

### Finding: Continuing in the Same Session (or Project) Is More Efficient than Fresh Chats (Due to Caching)

**Provider/Plan/Model(s):** Anthropic Claude (Claude Code, Pro/Max)  
**Source:** Anthropic Help Center  
**Evidence Type:** **Tier A** – official best-practice advice  
**Exact Quote/Screenshot:** _“Claude remembers context from earlier in the conversation… Content in projects is cached and doesn’t count against your limits when reused. Similar prompts you use frequently are partially cached… When you upload documents to a project, every time you reference that content, only new/uncached portions count against your limits.”_[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=Our%20system%20also%20includes%20caching,helps%20you%20optimize%20your%20limits)[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=Projects%20offer%20significant%20caching%20benefits%3A).

**Workload Context:** Multi-step feature pipeline: e.g. agent first plans (provides spec), then builds code, then reviews. If done in one continuous Claude chat or with a Project, the earlier spec can be referenced without re-cost. In a fresh session, the spec would need to be provided again from scratch (costing tokens twice).  
**Measurement:** _Example:_ Upload a 100 KB requirements file to a Project once – cost maybe 100 KB worth of tokens initially. Every subsequent prompt that draws on it incurs 0 tokens for that data (just a small lookup cost). Versus not using projects: each new chat would require re-uploading that file = repeatedly paying that 100 KB token cost.  
**Implications:** This directly affects the **$/feature efficiency**: keeping the conversation going (using `/continue` or just multi-turn dialogue) avoids re-sending context and therefore saves a lot of token quota[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=,against%20your%20limits%20when%20reused)[support.claude.com](https://support.claude.com/en/articles/9797557-usage-limit-best-practices#:~:text=,they%27re%20cached%20for%20future%20use). Similarly, using Claude’s “Projects” feature to store knowledge (files, previous answers) means subsequent steps can pull that info with minimal new tokens. In optimization terms, **stateful sessions have economies of scale** – the first prompt might be expensive (loading requirements, codebase context), but follow-ups are cheaper due to caching. This validates the idea that a methodology that can reuse context (e.g. plan once, then multiple build steps with that plan in memory) will burn less quota than one that repeatedly asks each model from scratch. The consistency assumption (units completed per % of bar) will be better for methods that leverage this caching because their token usage per feature is more predictable (less redundant payload).  
**Open Questions:** How long is session context cached? (Likely for the 5h session duration; after that, starting a new session even in same project may still reuse the project files cache though.) The help mentions “Claude’s memory” for Team/Enterprise that can even carry some info across chats – we have Pro/Max, which have manual search of past chats. For now, we should assume each 5h session is isolated, but within it we should chain tasks rather than restarting. Also, we should measure if the CLI `/continue` truly doesn’t add much overhead (likely just a short “Continue” prompt, negligible tokens). Finally, how much does using the CLI’s `--project` mode help in practice for caching file content? This might be an area for a quick experiment with identical tasks with and without project to see the bar difference.

### Finding: Claude’s Max Plans (5× and 20×) Scale Token Capacity Linearly – but Bugs Can Disrupt This

**Provider/Plan/Model(s):** Anthropic Claude (Pro vs Max 5× vs Max 20×)  
**Source:** Portkey blog & Anthropic issue tracker  
**Evidence Type:** **Tier A** – official pricing reference; **Tier B** – user-reported anomaly  
**Exact Quote/Screenshot:** _“Max 5× $100 = 5× Pro capacity… Max 20× $200 = 20× Pro… (Weekly limits scale proportionally. The 5-hour windows stay — weekly just adds a ceiling.)”_[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=Free%20%240%20%E2%89%88%20%E2%85%92%20Pro,Sonnet%204%20%2B%20Opus%204)[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=,stay%E2%80%94weekly%20just%20adds%20a%20ceiling). _“With Max 20x plan (24–40 hours of Opus 4 per week), the limit should last the full week… should not reach 76% by Tuesday… Observed: Tuesday morning 86% of weekly Opus limit… ~16% consumed in 30 minutes (expected ~0.3%). ‘This is clearly a bug, not normal consumption’.”_[github.com](https://github.com/anthropics/claude-code/issues/8449#:~:text=%2A%20Max%2020x%20%3D%2024,0.3%25%20%28not%2016)[github.com](https://github.com/anthropics/claude-code/issues/8449#:~:text=CRITICAL%20UPDATE%3A).

**Workload Context:** Under normal conditions: identical coding workload on Pro vs Max plans – Max 5× can handle ~5× more prompts per window. The bug report context: Claude Code v2, Max 20× user doing typical Rails app edits (nothing unusual) yet usage spiked.  
**Measurement:** _Plan capacities per 5h:_ **Pro:** ~10–40 prompts; **Max 5×:** ~50–200 prompts; **Max 20×:** ~200–800 prompts[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=Plan%20Monthly%20fee%20Relative%20capacity,Sonnet%204%20%2B%20Opus%204). _Token budgets:_ ~44k, ~88k, ~220k per 5h for Pro, 5×, 20× respectively[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=%2A%20According%20to%20third). The bug example saw ~16% of a weekly Opus cap (~approx 4–6 hours worth) burn in 30 minutes of moderate use[github.com](https://github.com/anthropics/claude-code/issues/8449#:~:text=Actual%20consumption%3A).  
**Implications:** In theory, the higher-tier Claude plans simply **multiply the available tokens** linearly. A Max 20× account should deliver roughly 20 times the “features per week” as a Pro account, assuming the methodology scales token usage linearly. Our decision to buy 1× Max 20 vs 4× Max 5 should on paper be neutral (both equal 20× Pro). There is **no inherent inefficiency at scale** according to Anthropic’s design – in fact the Portkey data confirms pricing is proportional[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=Free%20%240%20%E2%89%88%20%E2%85%92%20Pro,Sonnet%204%20%2B%20Opus%204). **However,** the bug indicates potential **violations of the consistency assumption**: after a major update, some Max users saw far more tokens counted than actually used, possibly due to double-counting or context replay issues[github.com](https://github.com/anthropics/claude-code/issues/8449#:~:text=Suspected%20issue%3A). This means the system must be vigilant: any abnormal jumps in %Δ per feature could signal a platform bug or hidden factor. For now, absent bugs, we expect **near-additive scaling**: two Max5 accounts together = one Max20 capacity. This allows flexibility: we could spread risk by having multiple smaller accounts instead of one large (if one hits a bug or goes down, others remain). The safety margin (we discussed leaving ~2% unused) is prudent especially seeing these anomalies – e.g. the user’s bar jumped by ~16% unexpectedly, so a buffer >10% is wise.  
**Open Questions:** Was the bug resolved? (Likely yes in subsequent Claude Code patches, but we should confirm via issue #8449 resolution if available.) It also raises the question: do **Max 20× accounts possibly suffer more from caching issues** (e.g. larger context or more parallel threads causing token recount)? In any case, we should implement monitoring of token usage vs expected, to catch if the real world deviates from linear scaling. If such issues persist, a strategy might be to favor multiple Max 5× accounts over a single Max 20×, even if cost is the same, just to sandbox the impact of a bug.

### Finding: One Claude Account’s Usage Pool Is Shared Across Parallel Sessions (Potential Throttling on Rapid Use)

**Provider/Plan/Model(s):** Anthropic Claude (Claude.ai web and Claude Code CLI)  
**Source:** User guide & ClaudeLog FAQ  
**Evidence Type:** **Tier B** – observed behavior and official community guidance  
**Exact Quote/Screenshot:** _“Your Claude.ai web chat and Claude Code share the SAME usage pool. Every prompt counts against the same budget. Starting a session on one starts the clock for both.”_[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=Fact%20,This%20Is%20The%20Key). _“Rate limiting: Temporary throttling during rapid requests, generally resolves itself.”_[claudelog.com](https://claudelog.com/faqs/claude-code-limit/#:~:text=Context%20Window%20%26%20Rate%20Limits).

**Workload Context:** Example – running two terminals with Claude Code on one account, or using the web UI and CLI simultaneously. If each sends prompts, they both draw down the single set of bars. If done concurrently at high speed, Anthropic may queue or throttle some requests.  
**Measurement:** If two parallel agents each try to use ~50% of the token budget in the same 5h window, they will hit the 100% cap combined (not each). Users note that exceeding a certain prompt rate causes Claude to slow responses or give “too many requests” until usage subsides (no hard numbers given, but likely a few requests per second at most).  
**Implications:** **Independence between providers** holds (Claude’s usage is independent of, say, OpenAI’s), but **not between sessions of the same provider account**. All sessions/logins of one Claude account funnel into one weekly and session quota[nathanonn.com](https://www.nathanonn.com/how-to-double-your-claude-code-usage-limits-without-upgrading-to-max/#:~:text=Your%20Claude,share%20the%20SAME%20usage%20pool). We cannot, for example, double capacity by opening two chats in parallel under one login – it will just consume the quota twice as fast. However, from a throughput perspective, parallelizing might still speed up completion of work before the 5h window closes (if that’s a factor). The note on throttling[claudelog.com](https://claudelog.com/faqs/claude-code-limit/#:~:text=Context%20Window%20%26%20Rate%20Limits) suggests Anthropic might protect against abuse by slowing responses if too many requests hit at once. This affects how we schedule tasks: if we try to run 8 parallel feature builds on one account in the same minute, some may be delayed. It likely doesn’t change the token accounting, just the latency. For optimization, it might be better to allocate one account per feature at a time (or ensure any parallel usage is moderate). If tasks overlap, they’ll simply slice the pie smaller per task. This finding reinforces that to scale throughput, we need multiple accounts (true multi-provider parallelism), not just multiple sessions on one.  
**Open Questions:** What is the **exact concurrency limit**? Anthropic doesn’t publish QPS limits for Claude.ai, but anecdotal evidence (and general API limits) suggest it’s low (maybe 1-2 prompts simultaneously). The CLI may queue internally if you spam it. We should test with two simultaneous requests to see if one gets a “please wait” response. Also, **does parallel usage affect accuracy?** Unlikely, but the context is separate per chat, so they won’t mix data. No interference beyond shared quota.

### Finding: ChatGPT (OpenAI) Uses Message Count Limits (e.g. 160/3h for GPT‑5 Plus) Instead of Token Bars

**Provider/Plan/Model(s):** OpenAI ChatGPT (GPT-4o, GPT-5 on Free, Plus, Pro plans)  
**Source:** OpenAI Help Center  
**Evidence Type:** **Tier A** – official usage policy (2025)  
**Exact Quote/Screenshot:** _“ChatGPT Free tier: up to 10 messages every 5 hours… Plus users can send up to 160 messages with GPT-5 every 3 hours (temporary increase, will revert to previous limit)… After reaching this limit, chats switch to the mini version until reset. Plus/Business can also use GPT-5 Thinking (deeper mode) up to 3,000 messages per week… Business and Pro offer unlimited access to GPT-5 models, subject to abuse guardrails.”_[help.openai.com](https://help.openai.com/en/articles/11909943-gpt-5-in-chatgpt#:~:text=Usage%20Limits)[help.openai.com](https://help.openai.com/en/articles/11909943-gpt-5-in-chatgpt#:~:text=If%20you%E2%80%99re%20on%20Plus%20or,be%20selectable%20from%20the%20menu).

**Workload Context:** Chat-based usage via chat.openai.com UI. E.g. a Plus user asking GPT-5 for coding help intensively can hit 160 messages within 3h, after which responses come from a reduced model until 3h passes. The Thinking mode (more powerful reasoning) has its own weekly cap.  
**Measurement:** _Free:_ ~10 messages/5h (~2 msgs/hour). _Plus:_ ~160 msgs/3h (~53 msgs/hour) – note this was a temporary doubling from a prior 80/3h. _GPT-5 Thinking:_ 3000 msgs/week cap on Plus/Bus. _Pro/Enterprise:_ effectively no preset cap, but “unlimited” gated by monitoring.  
**Implications:** Unlike Anthropic’s token-percent bars, OpenAI’s ChatGPT uses **discrete message limits in fixed time windows**[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=Everyone%20Counts%20Differently). This is a very different paradigm: it doesn’t directly show tokens, and the model auto-downgrades rather than outright blocking you when the cap is hit. For our cost optimization, if any methodology uses ChatGPT’s interface (or OpenAI’s chat API with similar constraints), we must measure in “messages” not hours. The independence assumption holds trivially here (OpenAI’s usage doesn’t affect Anthropic’s), but **additivity** in ChatGPT is more coarse-grained – 1 long message counts the same as 1 short message against the limit. However, if that long message hits token limits, it might fail; the help suggests an 8k token window for chat completions[github.com](https://github.com/orgs/community/discussions/133779#:~:text=Copilot%20chat%20token%20limit%20and,Rate%20Limits). There’s also a nuance: **auto-switching to GPT-5-mini** after hitting limit means continued usage is possible but at lower quality. So, for ChatGPT Plus, one could technically keep working beyond the 160-message limit, but results may degrade. The **weekly 3,000 Thinking messages** cap is akin to a heavy-use safe guard (roughly limiting how often you invoke the slow, reasoning-intensive mode). Notably, ChatGPT Pro (a higher tier, likely ~$50+/mo) offers _unmetered_ use for our purposes[help.openai.com](https://help.openai.com/en/articles/11909943-gpt-5-in-chatgpt#:~:text=weekly%20limit%2C%20and%20GPT,Thinking%20after%20you%E2%80%99ve%20reached%20it). “Unlimited” with abuse guardrails implies you won’t get cut off weekly, making it more predictable for sustained usage. We will need to factor this when comparing $/feature: an OpenAI Pro account might complete far more features than a Claude Max, since it has no hard weekly cap – its cost is purely monetary (API usage or flat fee).  
**Open Questions:** The ChatGPT usage model complicates direct comparison – how do we equate “160 messages/3h” to tokens or features? We might approximate tokens: if GPT-5’s context is large (32k on Plus), a user could theoretically pack a lot in one message. But practically, coding often involves iterative turns. We should observe how many messages it takes ChatGPT to implement a feature (maybe more efficient or fewer due to better code model?). Also, is ChatGPT’s 3h window rolling from first message or fixed (likely rolling per-user start, but OpenAI hasn’t specified – possibly it’s just any 3h interval sliding). For safety, treat it as sliding. Lastly, do the tools (code interpreter, etc.) count differently? The help doesn’t mention token costs for tools on ChatGPT UI (unlike API where they bill normally). Possibly the flat limits implicitly cover tool usage as well.

### Finding: OpenAI GPT‑5 API Pricing – Around $1.25 per 1K input tokens & $10 per 1K output (with Cheaper Mini/Nano Models)

**Provider/Plan/Model(s):** OpenAI API (GPT-5 family, 2025)  
**Source:** OpenAI official site  
**Evidence Type:** **Tier A** – official pricing table  
**Exact Quote/Screenshot:** _“GPT‑5… Input $1.25 | Output $10.00 per 1M tokens. GPT‑5 mini… Input $0.25 | Output $2.00 per 1M tokens. GPT‑5 nano… Input $0.05 | Output $0.40 per 1M tokens.”_[openai.com](https://openai.com/gpt-5/#:~:text=GPT,opens%20in%20a%20new%20window).

**Workload Context:** API usage (e.g. via OpenAI’s Python SDK or an agent environment like Cursor using GPT-5 models). These are pay-as-you-go costs, not the ChatGPT flat subscription.  
**Measurement:** _Normalized:_ GPT-5 = $0.00125 per 1K input token, $0.01 per 1K output. Mini = $0.00025 per 1K in, $0.002 per 1K out. (Nano extremely cheap at $0.00005 in, $0.0004 out.) For perspective, a typical coding prompt+answer of say 2K input + 4K output tokens on GPT-5 would cost about $0.005 + $0.04 = $0.045 each.  
**Implications:** If the user’s system will translate “% of weekly bar” into an equivalent dollar value, these are the anchors for OpenAI’s side. Notably, OpenAI’s pricing per token is **much lower** than what Anthropic’s implied costs are for their capped plans. For instance, GPT-5’s $10/M output = $0.01/1K, whereas Claude’s **Sonnet 4** via API is $15/1M out = $0.015/1K[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=Model%20Anthropic%20API%20AWS%20Bedrock,80%20%2F%20%244%20same%20same) (actually more expensive? Correction: Claude Sonnet is $15 per 1M out, so $0.015/1K, yes slightly more expensive output than GPT-5; and Claude Opus is $0.075/1K out – significantly higher). This means on a pure token-cost basis, GPT-5 is generally cheaper than Claude for equivalent token counts (especially considering GPT-5’s faster reasoning may use fewer tokens to reach a solution). However, the user’s _Claude subscription_ is a fixed sunk cost with “free” tokens up to the cap, whereas OpenAI API usage racks up costs linearly. We need these figures to decide if a methodology using OpenAI (pay per use) can beat the fixed-cost providers. For example, if Claude can ship X features per week for $200, can an OpenAI API approach deliver the same X features for less than $200 in token fees? The answer will depend on how many tokens per feature each model needs. A hybrid approach could be: use Claude up to its cap (since it’s already paid for) and spillover to GPT-5 API if needed, as GPT-5 has essentially unlimited availability if you pay. Also, GPT-5 Mini at 1/5 the cost could be analogous to using a weaker model for trivial tasks to save money (with trade-off in quality). This ties into the methodologies: maybe one pipeline uses only Claude (capped but “free” after subscription), another uses GPT-5 pay-go but maybe fewer reruns.  
**Open Questions:** We should confirm **Claude’s token analog**: the FrozenLight article implies Claude Max $200 roughly equals 220k tokens per 5h window[frozenlight.ai](https://www.frozenlight.ai/post/frozenlight/710/anthropic-sets-weekly-usage-limits-measured-in-hours/#:~:text=%2A%20According%20to%20third). That’s 220k in + out combined presumably. Spread over 7 days with maybe ~30 windows (if utilized continuously) = ~6.6M tokens/week. If one were to pay OpenAI for 6.6M GPT-5 tokens, input vs output cost matters. If half are output, that’s 3.3M * $10 + 3.3M * $1.25 = ~$37.5 + $4.125 = ~$41.6. Wait, $41 for 6.6M tokens of GPT-5, versus $200 Claude subscription. Of course, GPT-5 might need more tokens if it’s less efficient per feature, and Claude’s $200 covers up to maybe 10M+ tokens if fully used at lower models. But this rough math suggests why Anthropic has the caps: they don’t want a user consuming, say, 50M tokens for $200 (which would cost OpenAI ~$300). The user’s eventual ROI calculator will need these rates. Also, note OpenAI might have **reserved capacity or committed use discounts** for enterprise; possibly irrelevant here unless user can get such deals.

### Finding: Claude API Pricing – Sonnet 4 at ~$0.003 Input / $0.015 Output per 1K; Opus ~5× higher

**Provider/Plan/Model(s):** Anthropic Claude API (via Anthropic, AWS, or Google)  
**Source:** Portkey capacity guide (citing Anthropic pricing)  
**Evidence Type:** **Tier A** – official pricing (Anthropic published uniform prices)  
**Exact Quote/Screenshot:** _“Opus 4: $15 in / $75 out per M tokens… Sonnet 4: $3 in / $15 out per M… 3.5 Haiku: $0.80 / $4 per M… Anthropic confirms pricing parity across channels.”_[portkey.ai](https://portkey.ai/blog/claude-code-limits/#:~:text=Model%20Anthropic%20API%20AWS%20Bedrock,80%20%2F%20%244%20same%20same).

**Workload Context:** Using Claude via API (not the Claude.ai app). E.g., a developer calling Claude 4.1 through AWS Bedrock or Anthropic’s endpoint will pay these rates for usage beyond any free tier.  
**Measurement:** _Per 1K tokens:_ Opus 4 input $0.015, output $0.075. Sonnet 4 input $0.003, output $0.015. Haiku 3.5 input $0.0008, output $0.004. (We can interpret “Sonnet 4” as the 100k context 4.0 model used in Claude Code; “Claude 3.5 Haiku” is a lighter model, cheaper but presumably less capable).  
**Implications:** These rates allow us to put a dollar value on the Claude usage bars. For instance, if our Pro plan user roughly gets 44k tokens per 5h, that’s ~44k * ($0.003+$0.015)/2k (assuming roughly half in/out) ≈ $0.396 per session equivalent in API costs. Over a week (say 8 sessions/day *7=56 sessions), that’s ~$22 in token value – notably close to the $20 subscription, which checks out (Anthropic isn’t giving away much more than $20 worth of API usage!). For Max, the token allotment (~220k per 5h on 20×) would be ~$1.98 per session, or $111 per day if fully utilized, which would be ~$777/week – but they only charge $200 flat, hence why they needed to cap usage. This also highlights that **Claude Opus is expensive** – using a lot of Opus (at $0.075/1K out) can blow through budget quickly. Opus is ~5× the cost of Sonnet per token, reflecting why they cap its hours separately. In optimization, if a methodology heavily uses Opus, we should weigh if it’s truly needed or if Sonnet (cheaper) can do the job with perhaps more prompts. The **Haiku 3.5** price is very low – 80¢ in, $4 out per 1M – suggesting a strategy: use Haiku for simpler tasks or initial drafts, and only use the big models when necessary. This aligns with a multi-model pipeline idea: e.g., quick unit tests generation by a cheap model vs complex planning by an expensive one. Since the user has subscriptions, these API prices won’t be paid out-of-pocket during normal usage, but they represent the **opportunity cost** or “shadow price” of the subscription usage. We can thus convert observed % usage into notional dollar value and compare across providers.  
**Open Questions:** The **“Claude for Work Team/Enterprise”** plans mention extra usage – presumably, enterprise customers can pay for overages beyond weekly caps (like buying more tokens). What are those rates? Possibly they align with API prices. Also, do these prices change with model versions (Claude 4.1 vs 4.0)? Likely not significantly; Anthropic might adjust occasionally. For now, these are baseline. We should also note that these are **through API**; the user’s usage via the Claude app doesn’t directly charge these, but if usage was significantly above normal, Anthropic might force switching to API (not likely given the weekly caps now in place).

### Finding: Zhipu’s GLM-4.6 Coding Plan Delivers Claude-Level Performance at ~85% Lower Cost (with 3× Usage Quota)

**Provider/Plan/Model(s):** Zhipu AI Z.ai – GLM-4.6 Coding Plan  
**Source:** Zhipu AI official blog announcement  
**Evidence Type:** **Tier A** – official provider statement  
**Exact Quote/Screenshot:** _“For new users: The GLM Coding Plan offers Claude-level performance at a fraction of the cost — just 1/7th the price with 3× the usage quota. Start building today at $3/month.”_[z.ai](https://z.ai/blog/glm-4.6#:~:text=For%20New%20users%3A%20The%20GLM,ai%2Fsubscribe).

**Workload Context:** GLM-4.6 is a large language model available via Z.ai’s platform and can be used in tools like Claude Code (they mention integration). The Coding Plan likely targets similar coding assistant use cases, giving a pooled quota for GLM model usage.  
**Measurement:** _Relative to Claude:_ Price 14% (1/7th) of Claude Pro, Usage 300% of Claude Pro. In absolute terms, $3/month for what appears to be roughly 120–240 “Claude hours” (since Claude Pro ~40–80h). If accurate, $3 gets ~3×80 = ~240h of dev work/week capacity – which sounds extremely generous, possibly referring to GLM being more efficient per task. It might also be that $3 is the base for personal use; higher tiers may exist.  
**Implications:** This is a major competitor insight: GLM-4.6 claims near-parity with Claude Sonnet 4/4.5 on coding tasks[z.ai](https://z.ai/blog/glm-4.6#:~:text=We%20evaluated%20GLM,5%20in%20coding%20ability)[z.ai](https://z.ai/blog/glm-4.6#:~:text=and%20coding,5%20in%20coding%20ability), at a drastically lower cost. If true, the **$/feature** could be far lower with Z.ai, assuming the quality (bugs, rework rate) is acceptable. The “3× usage quota” suggests that if Claude Pro gives e.g. 40h/week, GLM plan gives ~120h/week of similar output. Perhaps GLM is more token-efficient (the Z.ai docs note _“GLM-4.6 is over 30% more efficient (fewer tokens) than GLM-4.5”_[docs.z.ai](https://docs.z.ai/guides/llm/glm-4.6#:~:text=that%20GLM,trajectories)). For our system, this means we should include GLM-4.6 pipelines in the evaluation – e.g., a methodology where planning, coding, reviewing are done with GLM-4.6 (or maybe planning with Claude and execution with GLM to mix strengths). The independence assumption holds (a Z.ai account’s usage doesn’t affect our Claude usage aside from maybe using the same client app), so we can run them in parallel. One caution: being cheaper could come with instability or limits not obvious in marketing. But as of now, Zhipu is positioning this to poach frustrated Claude users (especially those upset by new limits[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=Image%3A%20Cake%20icon%20%E2%80%A2%20,2d%20ago)). We should validate performance: the Z.ai blog claims wins on benchmarks and “near parity with Claude Sonnet 4 (48.6% win rate)”[z.ai](https://z.ai/blog/glm-4.6#:~:text=We%20evaluated%20GLM,5%20in%20coding%20ability) – meaning roughly a toss-up, which is promising. If GLM can indeed ship features with comparable quality, the cost optimization might heavily favor it (especially if Anthropic’s caps remain strict).  
**Open Questions:** How exactly is the “3× usage” measured? It could be 3× token quota or 3× hours of generation. Does Z.ai impose its own weekly or session limits? Possibly yes (3× usage implies there is a cap, just higher). The $3/month might be an introductory price or for individual use – an enterprise might cost more. Also, GLM-4.6’s context (200K) is huge, and if it’s more efficient, maybe it doesn’t need as many back-and-forth messages. We should gather some anecdotal results or run a trial if possible to confirm “cost per feature” using GLM. It may require adapting our tools (maybe through OpenRouter or their API).

### Finding: Claude’s Higher Reasoning Model (Opus 4.1) Produces Fewer Bugs and More Robust Code than GPT‑5 in Complex Tasks

**Provider/Plan/Model(s):** Anthropic Claude Opus vs OpenAI GPT-5 (latest models, coding scenario)  
**Source:** Independent benchmark blog (MPG One, Aug 2025)  
**Evidence Type:** **Tier B** – third-party comparative testing  
**Exact Quote/Screenshot:** _“Speed vs. Quality tradeoffs: GPT-5 is 30% faster in coding, but Claude Opus 4.1 produces less bugs… high price of Opus 4.1 comes with high quality and accuracy of coding.”_[mpgone.com](https://mpgone.com/gpt-5-vs-claude-opus-4-1/#:~:text=,enterprises%2C%20and%20specific%20development%20scenarios).

**Workload Context:** A head-to-head test on coding tasks (presumably writing and fixing code) measuring outcome quality, speed, and cost. Possibly involved debugging a piece of code or implementing features with each model and counting errors.  
**Measurement:** GPT-5 solved tasks ~30% quicker (time or fewer interactions) but the resulting code had more issues, whereas Claude Opus was slower but its solutions were nearly production-ready. “Less bugs” implies higher first-pass success rate or fewer needed corrections. No absolute bug counts given, but a qualitative difference observed. Also noted: GPT-5 cost ~37% less in tokens than Opus for the same tasks[mpgone.com](https://mpgone.com/gpt-5-vs-claude-opus-4-1/#:~:text=After%20OpenAI%20released%20GPT,on%20your%20needs%20and%20budget) (which aligns with pricing differences).  
**Implications:** This directly addresses the **quality vs cost** trade-off. If one methodology uses a stronger model in the planning or coding stage (e.g. Claude Opus for architecture or critical code), it might reduce the downstream rework (fewer test failures, fewer review edits). That can save time within the 5-hour windows and also means fewer iterative cycles, effectively allowing more features to ship. A cheaper model might appear to have a lower $/feature in raw throughput, but if each feature requires additional fixes, the real cost rises. This finding supports the idea that **“power upfront” pays off**: using Claude Opus in the planning or heavy reasoning parts could yield a correct implementation that Sonnet or GPT-5 then just executes. The user’s question about “rework multiplier” gets a positive answer here: yes, a more capable model can reduce rework significantly (perhaps by an order of magnitude in tricky scenarios, as one user on Reddit noted Sonnet 4.5 caused “collateral damage” and inefficiency compared to Opus[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=Since%20the%20new%20version%20update%2C,development%20efficiency%20has%20dropped%20tenfold)[reddit.com](https://www.reddit.com/r/ClaudeAI/comments/1nu9wew/usage_limits_discussion_megathread_beginning_sep/#:~:text=In%20my%20opinion%2C%20without%20Opus%2C,Not%20even%20%24100)). Our optimization system should thus incorporate a quality factor: e.g., if GPT-5 methodology has a 10% higher bug rate, that could translate to +10% time or cost per feature, negating a lot of token-cost savings. The elimination margin (τ = 10%) should consider not just cost in dollars or tokens but also quality-adjusted cost. We may run A/B tests for quality: measure how many test cases fail or how many review comments each methodology yields.  
**Open Questions:** The statement is broad – “less bugs” but what kind of bugs? Logic errors, syntax errors, integration issues? It mentions Claude produces well-commented code and edge-case considerations[mpgone.com](https://mpgone.com/gpt-5-vs-claude-opus-4-1/#:~:text=Claude%20Opus%204,that%20other%20models%20might%20miss), suggesting a thoroughness that might minimize certain classes of errors (e.g. handling nulls, adding tests). We should define _units shipped_ as passed all tests; perhaps Claude Opus might achieve that more often on first try. Also, GPT-5 being faster could mean faster response or fewer interactions needed – if time is a factor (though cost is main), that’s noted. If time-to-market is important, maybe one tolerates a few bugs for speed. However, since the user emphasizes provable cost, likely we prioritize cost per correct feature. Further research could include academic or industry studies on code-gen quality (OpenAI may have something on Codex vs human bug rates, and Anthropic might have internal evals). For now, we lean on this comparative result and any internal metrics we gather from our own runs.
