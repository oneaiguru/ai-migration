# Outcome Lab + Difficulty Model [Draft]

## ğŸ¯ Why Now
- Normalize quality across tasks/agents by tagging difficulty; improve fairness of provider comparisons.

## ğŸ”— Contracts (Depends, Emits)
- Depends: features ledger (quality), curated taskset
- Emits: difficulty tags, normalized scores, preview hint

## ğŸ§­ Diagram (Mermaid flowchart)
```mermaid
flowchart TD
  A[Golden taskset] --> B[Collect outcomes]
  B --> C[Fit difficulty (Elo/BT)]
  C --> D[Normalize scores]
  D --> E[Preview + stats]
```

## âœ… Acceptance
- A small taskset has difficulty scores; preview/stats use normalized quality.
- Documentation lists how to adjudicate outcomes consistently.

## ğŸ§ª Operator Quick Cue
- Command: `python -m quality.normalize --taskset docs/System/quality/outcome_lab_taskset.md`
- Check: preview shows normalized quality scores (e.g., `quality_norm=...`); ledger records difficulty tags

## â± Token Budget
- Estimate: 18K

## ğŸ›  Steps
1. Seed golden taskset + adjudication rubric.
2. Simple Elo/Bradleyâ€‘Terry fit with fallback.
3. Normalize preview and stats; tests for edge cases.

## âœ… Good Fit
- Works offline; adds clarity to experiments.

## ğŸš« Avoid
- Overâ€‘complex modeling; keep it simple and interpretable.

## ğŸ“ Links
- `docs/Backlog/experiment_designs.md`, `docs/Backlog/stats_power_ci.md`
